{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msps9341012/mygym/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from datamanager import DataManager\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10020 words not find in wordvector\n",
      "80721 words in total\n"
     ]
    }
   ],
   "source": [
    "datamanager = DataManager(\"/home/msps9341012/AGnews\")\n",
    "train_data, dev_data, test_data = datamanager.getdata(4, 199)\n",
    "wv = datamanager.get_wordvector(\"/home/msps9341012/AAAI18-code/glove.42B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=5\n",
    "max_lenth=199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_net:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.state = tf.placeholder(dtype=tf.float32,shape=[None,600], name='state')\n",
    "        self.input_x=tf.placeholder(dtype=tf.float32,shape=[None,300], name='input')\n",
    "        \n",
    "        self.actions = tf.placeholder(tf.int32, [None, ], name=\"actions_num\")\n",
    "        self.rewards=tf.placeholder(dtype=tf.float32,shape=[None], name='input')\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"ActorStep\")\n",
    "        self.learning_rate = tf.train.exponential_decay(0.0005, self.global_step, 10000, 0.95, staircase=True)\n",
    "        \n",
    "        with tf.variable_scope('policy_net'):\n",
    "            layer_1 = tf.layers.dense(inputs=self.state, units=1, activation=tf.identity)\n",
    "            layer_2 = tf.layers.dense(inputs=self.input_x, units=1, activation=tf.identity)\n",
    "            self.scaled_out=tf.sigmoid(layer_1+layer_2)\n",
    "            s_out = tf.clip_by_value(self.scaled_out, 1e-5, 1 - 1e-5)\n",
    "            self.scaled_out = tf.concat([1-s_out,s_out],axis=1) # Policy act_prob\n",
    "\n",
    "            self.act_stochastic = tf.multinomial(tf.log(self.scaled_out), num_samples=1) #[batch, n_class]\n",
    "            self.act_stochastic = tf.reshape(self.act_stochastic, shape=[-1])\n",
    "\n",
    "            self.act_deterministic = tf.argmax(self.scaled_out, axis=1)\n",
    "\n",
    "            self.scope = tf.get_variable_scope().name\n",
    "        #loss\n",
    "        neg_log_prob = tf.reduce_sum(-tf.log(self.scaled_out)*tf.one_hot(self.actions, 2), axis=1)\n",
    "        self.loss = tf.reduce_mean(neg_log_prob * self.rewards)\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss,global_step=self.global_step)\n",
    "            \n",
    "    def act(self,state,x,stochastic=True):\n",
    "        if stochastic:\n",
    "            return tf.get_default_session().run(self.act_stochastic, feed_dict={self.state: state,self.input_x:x})\n",
    "        else:\n",
    "            return tf.get_default_session().run(self.act_deterministic, feed_dict={self.state: state,self.input_x:x})\n",
    "\n",
    "    #def get_action_prob(self, obs):\n",
    "        #return tf.get_default_session().run(self.act_probs, feed_dict={self.obs: obs})\n",
    "\n",
    "    def train(self,state, x, actions, rewards):\n",
    "        return tf.get_default_session().run(self.train_op, feed_dict={self.state: state,\n",
    "                                                                      self.input_x:x,\n",
    "                                                                      self.actions: actions,\n",
    "                                                                      self.rewards: rewards})    \n",
    "    \n",
    "    def get_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.scope)\n",
    "\n",
    "    def get_trainable_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CriticNetwork():\n",
    "    def __init__(self, wordvector,dropout):\n",
    "        \n",
    "        self.dropout=0.5\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"LSTMStep\")\n",
    "        self.learning_rate = tf.train.exponential_decay(0.0005, self.global_step, 10000, 0.95, staircase=True)\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keepprob\")\n",
    "        self.seq = tf.placeholder(shape=[None, max_lenth], dtype=tf.int32, name=\"input_seq\") #一條\n",
    "        \n",
    "        self.cell_state=tf.placeholder(tf.float32, shape = [None, 600], name=\"cell_state\")\n",
    "        self.cell_input=tf.placeholder(tf.int32, shape = [None, 1], name=\"cell_input\")\n",
    "        \n",
    "        self.lenth = tf.placeholder(shape=[None], dtype=tf.int32, name=\"lenth\")\n",
    "        self.wordvector = tf.get_variable('wordvector', dtype=tf.float32, initializer=wordvector, trainable=True)\n",
    "        \n",
    "        self.ground_truth = tf.placeholder(tf.float32, [None,4], name=\"ground_truth\")\n",
    "        self.init = tf.random_uniform_initializer(-0.05, 0.05, dtype=tf.float32)\n",
    "        \n",
    "        self.seq_vec = tf.nn.embedding_lookup(self.wordvector, self.seq)\n",
    "        \n",
    "        self.vec = tf.nn.embedding_lookup(self.wordvector, self.cell_input) #for one step\n",
    "        \n",
    "        self.L2regular = 0.00001\n",
    "        \n",
    "        with tf.variable_scope('Critic'):\n",
    "            with tf.variable_scope('Rep') as scope:\n",
    "                self.cell = LSTMCell(300, initializer=self.init, state_is_tuple=False)\n",
    "                output, state=tf.nn.dynamic_rnn(self.cell,self.seq_vec,dtype=tf.float32,sequence_length=self.lenth)\n",
    "                output = tf.gather(output[0], self.lenth-1) #取出最後一個\n",
    "                scope.reuse_variables()\n",
    "                loss_norm_r=tf.nn.l2_loss(tf.get_variable('rnn/lstm_cell/kernel'))*self.L2regular\n",
    "                \n",
    "            with tf.variable_scope('CNet') as scope:\n",
    "                self.layer1 = tf.layers.dropout(output, self.keep_prob)   # drop out 50% of inputs\n",
    "                self.target_out = tf.layers.dense(self.layer1, 4) #output: 4\n",
    "                scope.reuse_variables()\n",
    "                loss_norm_c=tf.nn.l2_loss(tf.get_variable('dense/kernel'))*self.L2regular\n",
    "            \n",
    "            #one step            \n",
    "            self.output_next, self.h_next = self.cell.call(self.vec[:,0,:], self.cell_state)\n",
    "            self.scope = tf.get_variable_scope().name\n",
    "            \n",
    "        self.reward=tf.reduce_max(tf.multiply(tf.nn.softmax(self.target_out),self.ground_truth),axis=1)\n",
    "\n",
    "        self.loss_target = tf.nn.softmax_cross_entropy_with_logits(labels=self.ground_truth, logits=self.target_out)\n",
    "        \n",
    "        self.c_net_loss=self.loss_target+loss_norm_c\n",
    "        self.r_net_loss=self.loss_target+loss_norm_r\n",
    "        self.total_loss=self.loss_target+loss_norm_c+loss_norm_r\n",
    "        \n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss,global_step=self.global_step)\n",
    "        \n",
    "        self.pre_train_r=tf.train.AdamOptimizer(self.learning_rate).minimize(self.r_net_loss,global_step=self.global_step,\n",
    "                                                                             var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'Critic/Rep'))\n",
    "        \n",
    "        self.pre_train_c=tf.train.AdamOptimizer(self.learning_rate).minimize(self.c_net_loss,global_step=self.global_step,\n",
    "                                                                             var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'Critic/CNet'))\n",
    "        \n",
    "    def get_next_state(self,x,init_h):\n",
    "        return tf.get_default_session().run([self.output_next,self.h_next], feed_dict={self.cell_input: x,\n",
    "                                                                                      self.cell_state:init_h})\n",
    "    def get_reward(self,sequence,lenth,ground_truth):\n",
    "        return tf.get_default_session().run(self.reward, feed_dict={self.seq: sequence,\n",
    "                                                                    self.lenth: lenth,\n",
    "                                                                    self.ground_truth: ground_truth,\n",
    "                                                                    self.keep_prob: 1.0})\n",
    "    \n",
    "    def get_wordvect(self, sequence):\n",
    "        return tf.get_default_session().run(self.seq_vec, feed_dict={self.seq: sequence})\n",
    "    \n",
    "   \n",
    "    def train(self, sequence, lenth, ground_truth):\n",
    "        return tf.get_default_session().run([self.total_loss, self.train_op],feed_dict={self.seq: sequence,\n",
    "                                                                         self.lenth: lenth,\n",
    "                                                                         self.ground_truth: ground_truth,\n",
    "                                                                         self.keep_prob: self.dropout})\n",
    "    \n",
    "    def pretrain_c(self, sequence, lenth, ground_truth):\n",
    "        return tf.get_default_session().run([self.c_net_loss, self.pre_train_c],feed_dict={self.seq: sequence,\n",
    "                                                                            self.lenth: lenth,\n",
    "                                                                            self.ground_truth: ground_truth,\n",
    "                                                                            self.keep_prob: self.dropout})\n",
    "    \n",
    "    def pretrain_r(self, sequence, lenth, ground_truth):\n",
    "        return tf.get_default_session().run([self.r_net_loss, self.pre_train_r],feed_dict={self.seq: sequence,\n",
    "                                                                            self.lenth: lenth,\n",
    "                                                                            self.ground_truth: ground_truth,\n",
    "                                                                            self.keep_prob: self.dropout})\n",
    "    def get_trainable_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_RL(inputs, stochastic=True):\n",
    "    #print epsilon\n",
    "    current_lower_state = np.zeros((1, 2*300), dtype=np.float32) #initial state\n",
    "    actions = []\n",
    "    states = []\n",
    "    words=inputs['words']\n",
    "    vec=critic.get_wordvect([inputs['words']])[0]\n",
    "    lenth=inputs['lenth']\n",
    "    \n",
    "    #choose actions\n",
    "    for pos in range(lenth):\n",
    "        action = actor.act(current_lower_state, [vec[pos]],stochastic)\n",
    "        action=np.asscalar(action)\n",
    "        states.append([current_lower_state[0], vec[pos]]) #store state\n",
    "\n",
    "        actions.append(action)\n",
    "        if action == 1: # 1 for retain\n",
    "            out_d, current_lower_state = critic.get_next_state([[words[pos]]],current_lower_state) #可以進到一下步\n",
    "\n",
    "    Rinput = []\n",
    "    for (i, a) in enumerate(actions):\n",
    "        if a == 1:\n",
    "            Rinput.append(words[i])\n",
    "    Rlenth = len(Rinput)\n",
    "    if Rlenth == 0:\n",
    "        actions[lenth-2] = 1\n",
    "        Rinput.append(words[lenth-2])\n",
    "        Rlenth = 1\n",
    "    Rinput += [0] * (max_lenth - Rlenth) #補零\n",
    "    \n",
    "    return actions, states, Rinput, Rlenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7ffc2d092080>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msps9341012/mygym/lib/python3.6/site-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "actor=Policy_net()\n",
    "critic=LSTM_CriticNetwork(wv,0.5)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=np.array([inputs['solution'],inputs['solution']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([inputs['solution'],inputs['solution']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35294117647058826"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3529411764705882"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- float(Rlenth)/inputs['lenth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack(actionlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, states, Rinput, Rlenth = sampling_RL(inputs,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _ = critic.train([Rinput], [Rlenth], [inputs['solution']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionlist, statelist, inputlist, rewards = [], [], [],[]\n",
    "for inputs in datas:\n",
    "    actions, states, Rinput, Rlenth = sampling_RL(inputs,True)\n",
    "    actionlist.append(actions)\n",
    "    statelist.append(np.array(list(map(lambda x: x[0], states)))) #current_lower_state\n",
    "    inputlist.append(np.array(list(map(lambda x: x[1], states))))\n",
    "    \n",
    "    reward=np.asscalar(critic.get_reward([Rinput], [Rlenth],[inputs['solution']]))\n",
    "    reward += 0.05*4*(1- float(Rlenth)/inputs['lenth'])\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 600)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statelist[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 600)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(statelist).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 600)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(map(lambda x: x[0], states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actionlist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 199)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(map(lambda x: x['words'],datas))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_data)\n",
    "for b in range(len(train_data) // batchsize): #一個batch\n",
    "        datas = train_data[b * batchsize: (b+1) * batchsize]\n",
    "        totloss = 0.\n",
    "        actionlist, statelist, inputlist, rewards = [], [], [],[]\n",
    "        for inputs in datas:\n",
    "            actions, states, Rinput, Rlenth = sampling_RL(inputs,True)\n",
    "            actionlist.append(actions)\n",
    "            statelist.append(states\n",
    "            reward=np.asscalar(critic.get_reward([Rinput], [Rlenth],[inputs['solution']]))\n",
    "            reward += 0.05*4*(1- float(Rlenth)/inputs['lenth'])\n",
    "            rewards.append(reward)\n",
    "\n",
    "        loss, _ = critic.train([Rinput], [Rlenth], [solution])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(sess, actor, critic, train_data, batchsize, samplecnt=5, LSTM_trainable=True, RL_trainable=True):\n",
    "    print \"training : total \", len(train_data), \"nodes.\"\n",
    "    random.shuffle(train_data)\n",
    "    for b in range(len(train_data) / batchsize):\n",
    "        datas = train_data[b * batchsize: (b+1) * batchsize]\n",
    "        totloss = 0.\n",
    "        critic.assign_active_network()\n",
    "        actor.assign_active_network()\n",
    "        for j in range(batchsize):\n",
    "            #prepare\n",
    "            data = datas[j]\n",
    "            inputs, solution, lenth = data['words'], data['solution'], data['lenth']\n",
    "            #train the predict network\n",
    "            if RL_trainable:\n",
    "                actionlist, statelist, losslist = [], [], []\n",
    "                aveloss = 0.\n",
    "                for i in range(samplecnt):\n",
    "                    actions, states, Rinput, Rlenth = sampling_RL(sess, actor, inputs, critic.wordvector_find([inputs]), lenth, args.epsilon, Random=True)\n",
    "                    actionlist.append(actions)\n",
    "                    statelist.append(states)\n",
    "                    out, loss = critic.getloss([Rinput], [Rlenth], [solution])\n",
    "                    loss += (float(Rlenth) / lenth) **2 *0.15\n",
    "                    aveloss += loss\n",
    "                    losslist.append(loss)\n",
    "                \n",
    "                aveloss /= samplecnt\n",
    "                totloss += aveloss\n",
    "                grad = None\n",
    "                if LSTM_trainable:\n",
    "                    out, loss, _ = critic.train([Rinput], [Rlenth], [solution])\n",
    "                for i in range(samplecnt):\n",
    "                    for pos in range(len(actionlist[i])):\n",
    "                        rr = [0., 0.]\n",
    "                        rr[actionlist[i][pos]] = (losslist[i] - aveloss) * args.alpha\n",
    "                        g = actor.get_gradient(statelist[i][pos][0], statelist[i][pos][1], rr)\n",
    "                        if grad == None:\n",
    "                            grad = g\n",
    "                        else:\n",
    "                            grad[0] += g[0]\n",
    "                            grad[1] += g[1]\n",
    "                            grad[2] += g[2]\n",
    "                actor.train(grad)\n",
    "            else: #Pretrain Critic 用\n",
    "                out, loss, _ = critic.train([inputs], [lenth], [solution])\n",
    "                totloss += loss\n",
    "        if RL_trainable:\n",
    "            actor.update_target_network()\n",
    "            if LSTM_trainable:\n",
    "                critic.update_target_network()\n",
    "        else:\n",
    "            critic.assign_target_network()\n",
    "        if (b + 1) % 500 == 0:\n",
    "            acc_test = test(sess, actor, critic, test_data, noRL= not RL_trainable)\n",
    "            acc_dev = test(sess, actor, critic, dev_data, noRL= not RL_trainable)\n",
    "            print(\"batch \",b , \"total loss \", totloss, \"----test: \", acc_test, \"| dev: \", acc_dev)\n",
    "\n",
    "\n",
    "def test(sess, actor, critic, test_data, noRL=False):\n",
    "    acc = 0\n",
    "    for i in range(len(test_data)):\n",
    "        #prepare\n",
    "        data = test_data[i]\n",
    "        inputs, solution, lenth = data['words'], data['solution'], data['lenth']\n",
    "        \n",
    "        #predict\n",
    "        if noRL:\n",
    "            out = critic.predict_target([inputs], [lenth])\n",
    "        else:\n",
    "            actions, states, Rinput, Rlenth = sampling_RL(sess, actor, inputs, critic.wordvector_find([inputs]), lenth, Random=False)\n",
    "            out = critic.predict_target([Rinput], [Rlenth])\n",
    "        if np.argmax(out) == np.argmax(solution):\n",
    "            acc += 1\n",
    "    return float(acc) / len(test_data)\n",
    "\n",
    "\n",
    "### Main ###\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config = config) as sess:\n",
    "    #model\n",
    "    critic = LSTM_CriticNetwork(sess, args.dim, args.optimizer, args.lr, args.tau, args.grained, args.maxlenth, args.dropout, word_vector) \n",
    "    actor = ActorNetwork(sess, args.dim, args.optimizer, args.lr, args.tau)\n",
    "    #print variables\n",
    "    for item in tf.trainable_variables():\n",
    "        print (item.name, item.get_shape())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #LSTM pretrain\n",
    "    if args.RLpretrain != '':\n",
    "        pass\n",
    "    elif args.LSTMpretrain == '':\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(0, 2):\n",
    "            train(sess, actor, critic, train_data, args.batchsize, args.sample_cnt, RL_trainable=False)\n",
    "            critic.assign_target_network()\n",
    "            acc_test = test(sess, actor, critic, test_data, True)\n",
    "            acc_dev = test(sess, actor, critic, dev_data, True)\n",
    "            print (\"LSTM_only \",i, \"----test: \", acc_test, \"| dev: \", acc_dev)\n",
    "            saver.save(sess, \"checkpoints/\"+args.name+\"_base\", global_step=i)\n",
    "        print (\"LSTM pretrain OK\")\n",
    "    else:\n",
    "        print (\"Load LSTM from \", args.LSTMpretrain)\n",
    "        saver.restore(sess, args.LSTMpretrain)\n",
    "    \n",
    "    print (\"epsilon\", args.epsilon)\n",
    "\n",
    "    if args.RLpretrain == '':\n",
    "        for i in range(0, 5):\n",
    "            train(sess, actor, critic, train_data, args.batchsize, args.sample_cnt, LSTM_trainable=False)\n",
    "            acc_test = test(sess, actor, critic, test_data)\n",
    "            acc_dev = test(sess, actor, critic, dev_data)\n",
    "            print(\"RL pretrain \", i, \"----test: \", acc_test, \"| dev: \", acc_dev)\n",
    "            saver.save(sess, \"checkpoints/\"+args.name+\"_RLpre\", global_step=i)\n",
    "        print (\"RL pretrain OK\")\n",
    "    else:\n",
    "        print (\"Load RL from\", args.RLpretrain)\n",
    "        saver.restore(sess, args.RLpretrain)\n",
    "\n",
    "    for e in range(args.epoch):\n",
    "        train(sess, actor, critic, train_data, args.batchsize, args.sample_cnt)\n",
    "        acc_test = test(sess, actor, critic, test_data)\n",
    "        acc_dev = test(sess, actor, critic, dev_data)\n",
    "        print (\"epoch \", e, \"----test: \", acc_test, \"| dev: \", acc_dev)\n",
    "        saver.save(sess, \"checkpoints/\"+args.name, global_step=e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
