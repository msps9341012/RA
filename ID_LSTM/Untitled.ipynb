{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msps9341012/mygym/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from datamanager import DataManager\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10020 words not find in wordvector\n",
      "80721 words in total\n"
     ]
    }
   ],
   "source": [
    "datamanager = DataManager(\"/home/msps9341012/AGnews\")\n",
    "train_data, dev_data, test_data = datamanager.getdata(4, 199)\n",
    "wv = datamanager.get_wordvector(\"/home/msps9341012/AAAI18-code/glove.42B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word=np.vstack(list(map(lambda x: x['words'],test_data)))\n",
    "test_lenth=np.hstack(list(map(lambda x: x['lenth'],test_data)))\n",
    "test_solution=np.vstack(list(map(lambda x: x['solution'],test_data)))\n",
    "\n",
    "dev_word=np.vstack(list(map(lambda x: x['words'],dev_data)))\n",
    "dev_lenth=np.hstack(list(map(lambda x: x['lenth'],dev_data)))\n",
    "dev_solution=np.vstack(list(map(lambda x: x['solution'],dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=5\n",
    "max_lenth=199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_net:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.state = tf.placeholder(dtype=tf.float32,shape=[None,600], name='state')\n",
    "        self.input_x=tf.placeholder(dtype=tf.float32,shape=[None,300], name='input')\n",
    "        \n",
    "        self.actions = tf.placeholder(tf.int32, [None, ], name=\"actions_num\")\n",
    "        self.rewards=tf.placeholder(dtype=tf.float32,shape=[None], name='input')\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"ActorStep\")\n",
    "        self.learning_rate = tf.train.exponential_decay(0.0005, self.global_step, 10000, 0.95, staircase=True)\n",
    "        \n",
    "        with tf.variable_scope('policy_net'):\n",
    "            layer_1 = tf.layers.dense(inputs=self.state, units=1, activation=tf.identity)\n",
    "            layer_2 = tf.layers.dense(inputs=self.input_x, units=1, activation=tf.identity)\n",
    "            self.scaled_out=tf.sigmoid(layer_1+layer_2)\n",
    "            s_out = tf.clip_by_value(self.scaled_out, 1e-5, 1 - 1e-5)\n",
    "            self.scaled_out = tf.concat([1-s_out,s_out],axis=1) # Policy act_prob\n",
    "\n",
    "            self.act_stochastic = tf.multinomial(tf.log(self.scaled_out), num_samples=1) #[batch, n_class]\n",
    "            self.act_stochastic = tf.reshape(self.act_stochastic, shape=[-1])\n",
    "\n",
    "            self.act_deterministic = tf.argmax(self.scaled_out, axis=1)\n",
    "\n",
    "            self.scope = tf.get_variable_scope().name\n",
    "        #loss\n",
    "        neg_log_prob = tf.reduce_sum(-tf.log(self.scaled_out)*tf.one_hot(self.actions, 2), axis=1)\n",
    "        self.loss = tf.reduce_mean(neg_log_prob * self.rewards)\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss,global_step=self.global_step)\n",
    "            \n",
    "    def act(self,state,x,stochastic=True):\n",
    "        if stochastic:\n",
    "            return tf.get_default_session().run(self.act_stochastic, feed_dict={self.state: state,self.input_x:x})\n",
    "        else:\n",
    "            return tf.get_default_session().run(self.act_deterministic, feed_dict={self.state: state,self.input_x:x})\n",
    "\n",
    "    #def get_action_prob(self, obs):\n",
    "        #return tf.get_default_session().run(self.act_probs, feed_dict={self.obs: obs})\n",
    "\n",
    "    def train(self,state, x, actions, rewards):\n",
    "        return tf.get_default_session().run([self.loss,self.train_op], feed_dict={self.state: state,\n",
    "                                                                      self.input_x:x,\n",
    "                                                                      self.actions: actions,\n",
    "                                                                      self.rewards: rewards})    \n",
    "    \n",
    "    def get_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.scope)\n",
    "\n",
    "    def get_trainable_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CriticNetwork():\n",
    "    def __init__(self, wordvector,dropout):\n",
    "        with tf.variable_scope('Critic'):\n",
    "            self.dropout=0.5\n",
    "            self.global_step = tf.Variable(0, trainable=False, name=\"LSTMStep\")\n",
    "            self.learning_rate = tf.train.exponential_decay(0.0005, self.global_step, 10000, 0.95, staircase=True)\n",
    "\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name=\"keepprob\")\n",
    "            self.seq = tf.placeholder(shape=[None, max_lenth], dtype=tf.int32, name=\"input_seq\") #一條\n",
    "\n",
    "            self.cell_state=tf.placeholder(tf.float32, shape = [None, 600], name=\"cell_state\")\n",
    "            self.cell_input=tf.placeholder(tf.int32, shape = [None, 1], name=\"cell_input\")\n",
    "\n",
    "            self.lenth = tf.placeholder(shape=[None], dtype=tf.int32, name=\"lenth\")\n",
    "            self.wordvector = tf.get_variable('wordvector', dtype=tf.float32, initializer=wordvector, trainable=True)\n",
    "\n",
    "            self.ground_truth = tf.placeholder(tf.float32, [None,4], name=\"ground_truth\")\n",
    "            self.init = tf.random_uniform_initializer(-0.05, 0.05, dtype=tf.float32)\n",
    "\n",
    "            self.seq_vec = tf.nn.embedding_lookup(self.wordvector, self.seq)\n",
    "\n",
    "            self.vec = tf.nn.embedding_lookup(self.wordvector, self.cell_input) #for one step\n",
    "\n",
    "            self.L2regular = 0.00001\n",
    "        \n",
    "            with tf.variable_scope('Rep') as scope:\n",
    "                self.cell = LSTMCell(300, initializer=self.init, state_is_tuple=False)\n",
    "                self.output, state=tf.nn.dynamic_rnn(self.cell,self.seq_vec,dtype=tf.float32,sequence_length=self.lenth)\n",
    "                batch_range = tf.range(tf.shape(self.lenth)[0])\n",
    "                self.output=tf.gather_nd(self.output, tf.stack([batch_range, self.lenth-1], axis=1))#取出最後一個\n",
    "                scope.reuse_variables()\n",
    "                loss_norm_r=tf.nn.l2_loss(tf.get_variable('rnn/lstm_cell/kernel'))*self.L2regular\n",
    "                \n",
    "            with tf.variable_scope('CNet') as scope:\n",
    "                self.layer1 = tf.layers.dropout(self.output, self.keep_prob)   # drop out 50% of inputs\n",
    "                self.target_out = tf.layers.dense(self.layer1, 4) #output: 4\n",
    "                scope.reuse_variables()\n",
    "                loss_norm_c=tf.nn.l2_loss(tf.get_variable('dense/kernel'))*self.L2regular\n",
    "            \n",
    "            #one step            \n",
    "            self.output_next, self.h_next = self.cell.call(self.vec[:,0,:], self.cell_state)\n",
    "            self.scope = tf.get_variable_scope().name\n",
    "            \n",
    "        self.reward=tf.reduce_max(tf.multiply(tf.nn.softmax(self.target_out),self.ground_truth),axis=1)\n",
    "\n",
    "        self.loss_target = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.ground_truth, logits=self.target_out))\n",
    "        \n",
    "        self.c_net_loss=self.loss_target+loss_norm_c\n",
    "        self.r_net_loss=self.loss_target+loss_norm_r\n",
    "        self.total_loss=self.loss_target+loss_norm_c+loss_norm_r\n",
    "        \n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss,global_step=self.global_step)\n",
    "        \n",
    "        self.pre_train_r=tf.train.AdamOptimizer(self.learning_rate).minimize(self.r_net_loss,global_step=self.global_step,\n",
    "                                                                             var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'Critic/Rep'))\n",
    "        \n",
    "        self.pre_train_c=tf.train.AdamOptimizer(self.learning_rate).minimize(self.c_net_loss,global_step=self.global_step,\n",
    "                                                                             var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'Critic/CNet'))\n",
    "        \n",
    "    def get_next_state(self,x,init_h):\n",
    "        return tf.get_default_session().run([self.output_next,self.h_next], feed_dict={self.cell_input: x,\n",
    "                                                                                      self.cell_state:init_h})\n",
    "    def get_reward(self,sequence,lenth,ground_truth):\n",
    "        return tf.get_default_session().run(self.reward, feed_dict={self.seq: sequence,\n",
    "                                                                    self.lenth: lenth,\n",
    "                                                                    self.ground_truth: ground_truth,\n",
    "                                                                    self.keep_prob: 1.0})\n",
    "    \n",
    "    def get_wordvect(self, sequence):\n",
    "        return tf.get_default_session().run(self.seq_vec, feed_dict={self.seq: sequence})\n",
    "    \n",
    "   \n",
    "    def train(self, sequence, lenth, ground_truth):\n",
    "        return tf.get_default_session().run([self.total_loss, self.train_op],feed_dict={self.seq: sequence,\n",
    "                                                                         self.lenth: lenth,\n",
    "                                                                         self.ground_truth: ground_truth,\n",
    "                                                                         self.keep_prob: self.dropout})\n",
    "    def predict(self, sequence, lenth):\n",
    "        return tf.get_default_session().run(self.target_out, feed_dict={self.seq: sequence,\n",
    "                                                         self.lenth: lenth,\n",
    "                                                         self.keep_prob: 1.0})\n",
    "    \n",
    "    def pretrain_c(self, sequence, lenth, ground_truth):\n",
    "        return tf.get_default_session().run([self.c_net_loss, self.pre_train_c],feed_dict={self.seq: sequence,\n",
    "                                                                            self.lenth: lenth,\n",
    "                                                                            self.ground_truth: ground_truth,\n",
    "                                                                            self.keep_prob: self.dropout})\n",
    "    \n",
    "    def pretrain_r(self, sequence, lenth, ground_truth):\n",
    "        return tf.get_default_session().run([self.r_net_loss, self.pre_train_r],feed_dict={self.seq: sequence,\n",
    "                                                                            self.lenth: lenth,\n",
    "                                                                            self.ground_truth: ground_truth,\n",
    "                                                                            self.keep_prob: self.dropout})\n",
    "    def get_trainable_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_RL(inputs, stochastic=True):\n",
    "    #print epsilon\n",
    "    current_lower_state = np.zeros((1, 2*300), dtype=np.float32) #initial state\n",
    "    actions = []\n",
    "    states = []\n",
    "    words=inputs['words']\n",
    "    vec=critic.get_wordvect([inputs['words']])[0]\n",
    "    lenth=inputs['lenth']\n",
    "    \n",
    "    #choose actions\n",
    "    for pos in range(lenth):\n",
    "        action = actor.act(current_lower_state, [vec[pos]],stochastic)\n",
    "        action=np.asscalar(action)\n",
    "        states.append([current_lower_state[0], vec[pos]]) #store state\n",
    "\n",
    "        actions.append(action)\n",
    "        if action == 1: # 1 for retain\n",
    "            out_d, current_lower_state = critic.get_next_state([[words[pos]]],current_lower_state) #可以進到一下步\n",
    "\n",
    "    Rinput = []\n",
    "    for (i, a) in enumerate(actions):\n",
    "        if a == 1:\n",
    "            Rinput.append(words[i])\n",
    "    Rlenth = len(Rinput)\n",
    "    if Rlenth == 0:\n",
    "        actions[lenth-2] = 1\n",
    "        Rinput.append(words[lenth-2])\n",
    "        Rlenth = 1\n",
    "    Rinput += [0] * (max_lenth - Rlenth) #補零\n",
    "    \n",
    "    return actions, states, Rinput, Rlenth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7ffc70200f60>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msps9341012/mygym/lib/python3.6/site-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "actor=Policy_net()\n",
    "critic=LSTM_CriticNetwork(wv,0.5)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data,test_word,test_lenth,test_solution,noRL):\n",
    "    acc = 0\n",
    "    if noRL:\n",
    "        for b in range(len(test_word) // 100):\n",
    "            out = critic.predict(test_word[b * 100: (b+1) * 100], test_lenth[b * 100: (b+1) * 100])\n",
    "            acc=acc+np.sum(np.argmax(out,axis=1)==np.argmax(test_solution[b*100:(b+1) * 100],axis=1))\n",
    "            \n",
    "    else:        \n",
    "        for data in test_data:         \n",
    "            actions, states, Rinput, Rlenth = sampling_RL(data,False)\n",
    "            out = critic.predict([Rinput], [Rlenth])\n",
    "            if np.argmax(out) == np.argmax(data['solution']):\n",
    "                acc += 1\n",
    "    return float(acc) / len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_data)\n",
    "for b in range(len(train_data) // batchsize): #一個batch\n",
    "    datas = train_data[b * batchsize: (b+1) * batchsize]\n",
    "    total_loss=0\n",
    "    for inputs in datas: \n",
    "        #if net=='cnet':\n",
    "        loss, _ = critic.pretrain_c([inputs['words']], [inputs['lenth']], [inputs['solution']])\n",
    "        total_loss=total_loss+loss\n",
    "        #if net=='rep':\n",
    "            #loss, _ = critic.pretrain_r(word_batch, lenth_batch, solution_batch)\n",
    "        if (b+1)%1000==0:    \n",
    "            acc_test = test(test_data,test_word,test_lenth,test_solution, noRL= True)\n",
    "            acc_dev = test(dev_data,dev_word,dev_lenth,dev_solution, noRL= True)\n",
    "            print(\"batch \",b , \"total loss \", loss, \"----test: \", acc_test, \"| dev: \", acc_dev)\n",
    "    else:\n",
    "        actionlist, statelist, inputlist, rewards = [], [], [],[]\n",
    "        for inputs in datas: #batch 一條一條跑\n",
    "            actions, states, Rinput, Rlenth = sampling_RL(inputs,True)\n",
    "            statelist.append(np.array(list(map(lambda x: x[0], states)))) #current_lower_state\n",
    "            inputlist.append(np.array(list(map(lambda x: x[1], states))))\n",
    "            reward=np.asscalar(critic.get_reward([Rinput], [Rlenth],[inputs['solution']]))\n",
    "            reward += 0.05*4*(1- float(Rlenth)/inputs['lenth'])\n",
    "            rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch  499 total loss  1.927728 ----test:  0.8475 | dev:  0.8442\n",
      "batch  999 total loss  0.40529075 ----test:  0.8640789473684211 | dev:  0.8621\n",
      "batch  1499 total loss  0.16963314 ----test:  0.8832894736842105 | dev:  0.8812\n",
      "batch  1999 total loss  0.02860642 ----test:  0.8869736842105264 | dev:  0.8857\n",
      "batch  2499 total loss  0.04875701 ----test:  0.89 | dev:  0.8904\n",
      "batch  2999 total loss  0.10487152 ----test:  0.8905263157894737 | dev:  0.893\n",
      "batch  3499 total loss  0.81759316 ----test:  0.8869736842105264 | dev:  0.8886\n",
      "batch  3999 total loss  0.04382562 ----test:  0.8803947368421052 | dev:  0.8816\n",
      "batch  4499 total loss  1.3453445 ----test:  0.895 | dev:  0.8968\n",
      "batch  4999 total loss  0.98137546 ----test:  0.8935526315789474 | dev:  0.8949\n",
      "batch  5499 total loss  0.88240844 ----test:  0.9023684210526316 | dev:  0.9\n",
      "batch  5999 total loss  0.18125913 ----test:  0.9040789473684211 | dev:  0.9038\n",
      "batch  6499 total loss  0.016127413 ----test:  0.9048684210526315 | dev:  0.9081\n",
      "batch  6999 total loss  1.192561 ----test:  0.9068421052631579 | dev:  0.9087\n",
      "batch  7499 total loss  0.024256714 ----test:  0.9006578947368421 | dev:  0.9013\n",
      "batch  7999 total loss  0.081471644 ----test:  0.9072368421052631 | dev:  0.9081\n",
      "batch  8499 total loss  0.15304086 ----test:  0.9069736842105263 | dev:  0.9097\n",
      "batch  8999 total loss  0.023040222 ----test:  0.9103947368421053 | dev:  0.9129\n",
      "batch  9499 total loss  0.029423857 ----test:  0.9101315789473684 | dev:  0.914\n",
      "batch  9999 total loss  0.14757933 ----test:  0.9121052631578948 | dev:  0.9141\n",
      "batch  10499 total loss  0.1373971 ----test:  0.9093421052631578 | dev:  0.9125\n",
      "batch  10999 total loss  1.1084547 ----test:  0.9088157894736842 | dev:  0.9138\n",
      "batch  11499 total loss  0.13545054 ----test:  0.9114473684210527 | dev:  0.9128\n",
      "batch  11999 total loss  0.33697107 ----test:  0.9169736842105263 | dev:  0.9185\n",
      "batch  12499 total loss  0.034669176 ----test:  0.9163157894736842 | dev:  0.917\n",
      "batch  12999 total loss  0.20578298 ----test:  0.9140789473684211 | dev:  0.9177\n",
      "batch  13499 total loss  0.09757361 ----test:  0.9181578947368421 | dev:  0.9215\n",
      "batch  13999 total loss  0.45927563 ----test:  0.9172368421052631 | dev:  0.9205\n",
      "batch  14499 total loss  0.0688858 ----test:  0.9139473684210526 | dev:  0.9187\n",
      "batch  14999 total loss  0.034644704 ----test:  0.9198684210526316 | dev:  0.9212\n",
      "batch  15499 total loss  0.07422032 ----test:  0.9184210526315789 | dev:  0.9167\n",
      "batch  15999 total loss  0.09550141 ----test:  0.92 | dev:  0.9246\n",
      "batch  16499 total loss  0.11334565 ----test:  0.9153947368421053 | dev:  0.9178\n",
      "batch  16999 total loss  0.021870825 ----test:  0.92 | dev:  0.9223\n"
     ]
    }
   ],
   "source": [
    "pretrain('cnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(net):\n",
    "    random.shuffle(train_data)\n",
    "    for b in range(len(train_data) // batchsize): #一個batch\n",
    "        datas = train_data[b * batchsize: (b+1) * batchsize]\n",
    "        if net =='cnet':\n",
    "            word_batch=np.vstack(list(map(lambda x: x['words'],datas)))\n",
    "            lenth_batch=np.hstack(list(map(lambda x: x['lenth'],datas)))\n",
    "            solution_batch=np.vstack(list(map(lambda x: x['solution'],datas)))\n",
    "            loss, _ = critic.train(word_batch, lenth_batch, solution_batch)\n",
    "            if (b+1)%500==0:    \n",
    "                acc_test = test(test_data,test_word,test_lenth,test_solution, noRL= True)\n",
    "                acc_dev = test(dev_data,dev_word,dev_lenth,dev_solution, noRL= True)\n",
    "                print(\"batch \",b , \"total loss \", loss, \"----test: \", acc_test, \"| dev: \", acc_dev)\n",
    "        else:\n",
    "            actionlist, statelist, inputlist, rewards = [], [], [],[]\n",
    "            for inputs in datas: #batch 一條一條跑\n",
    "                actions, states, Rinput, Rlenth = sampling_RL(inputs,True)\n",
    "                statelist.append(np.array(list(map(lambda x: x[0], states)))) #current_lower_state\n",
    "                inputlist.append(np.array(list(map(lambda x: x[1], states))))\n",
    "                reward=np.asscalar(critic.get_reward([Rinput], [Rlenth],[inputs['solution']]))\n",
    "                reward += 0.05*4*(1- float(Rlenth)/inputs['lenth'])\n",
    "                rewards.append(reward)\n",
    "            \n",
    "            loss, _=actor.train(np.vstack(statelist),np.vstack(inputlist),np.hstack(actionlist),np.hstack(rewards)) \n",
    "            if (b+1)%1000==0:    \n",
    "                acc_test = test(test_data, noRL= False)\n",
    "                acc_dev = test(dev_data, noRL= False)\n",
    "                print(\"batch \",b , \"total loss \", loss, \"----test: \", acc_test, \"| dev: \", acc_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main\n",
    "print('---Main Train---')\n",
    "random.shuffle(train_data)\n",
    "for b in range(len(train_data) // batchsize): #一個batch\n",
    "        datas = train_data[b * batchsize: (b+1) * batchsize]\n",
    "        totloss = 0.\n",
    "        actionlist, statelist, inputlist, rewards = [], [], [],[]\n",
    "        Rinput_list=[]\n",
    "        Rlenth_list=[]\n",
    "        solution_list=[]\n",
    "        for inputs in datas:\n",
    "            actions, states, Rinput, Rlenth = sampling_RL(inputs,True)\n",
    "            Rinput_list.append(Rinput)\n",
    "            Rlenth_list.append(Rlenth)\n",
    "            solution_list.append(solution_list)\n",
    "            actionlist.append(actions)\n",
    "            statelist.append(states)\n",
    "            reward=np.asscalar(critic.get_reward([Rinput], [Rlenth],[inputs['solution']]))\n",
    "            reward += 0.05*4*(1- float(Rlenth)/inputs['lenth'])\n",
    "            rewards.append(reward)\n",
    "            \n",
    "        critic_loss, _ = critic.train(np.vstack(Rinput_list), np.hstack(Rlenth_list), np.vstack(solution_list))\n",
    "        \n",
    "        actor_loss, _=actor.train(np.vstack(statelist),np.vstack(inputlist),np.hstack(actionlist),np.hstack(rewards))\n",
    "                             \n",
    "        if (b+1)%1000==0:\n",
    "            acc_test = test(test_data, noRL= False)\n",
    "            acc_dev = test(dev_data, noRL= False)\n",
    "            print(\"batch \",b , \"total loss \", critic_loss+actor_loss, \"----test: \", acc_test, \"| dev: \", acc_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(sess, actor, critic, train_data, batchsize, samplecnt=5, LSTM_trainable=True, RL_trainable=True):\n",
    "    print \"training : total \", len(train_data), \"nodes.\"\n",
    "    random.shuffle(train_data)\n",
    "    for b in range(len(train_data) / batchsize):\n",
    "        datas = train_data[b * batchsize: (b+1) * batchsize]\n",
    "        totloss = 0.\n",
    "        critic.assign_active_network()\n",
    "        actor.assign_active_network()\n",
    "        for j in range(batchsize):\n",
    "            #prepare\n",
    "            data = datas[j]\n",
    "            inputs, solution, lenth = data['words'], data['solution'], data['lenth']\n",
    "            #train the predict network\n",
    "            if RL_trainable:\n",
    "                actionlist, statelist, losslist = [], [], []\n",
    "                aveloss = 0.\n",
    "                for i in range(samplecnt):\n",
    "                    actions, states, Rinput, Rlenth = sampling_RL(sess, actor, inputs, critic.wordvector_find([inputs]), lenth, args.epsilon, Random=True)\n",
    "                    actionlist.append(actions)\n",
    "                    statelist.append(states)\n",
    "                    out, loss = critic.getloss([Rinput], [Rlenth], [solution])\n",
    "                    loss += (float(Rlenth) / lenth) **2 *0.15\n",
    "                    aveloss += loss\n",
    "                    losslist.append(loss)\n",
    "                \n",
    "                aveloss /= samplecnt\n",
    "                totloss += aveloss\n",
    "                grad = None\n",
    "                if LSTM_trainable:\n",
    "                    out, loss, _ = critic.train([Rinput], [Rlenth], [solution])\n",
    "                for i in range(samplecnt):\n",
    "                    for pos in range(len(actionlist[i])):\n",
    "                        rr = [0., 0.]\n",
    "                        rr[actionlist[i][pos]] = (losslist[i] - aveloss) * args.alpha\n",
    "                        g = actor.get_gradient(statelist[i][pos][0], statelist[i][pos][1], rr)\n",
    "                        if grad == None:\n",
    "                            grad = g\n",
    "                        else:\n",
    "                            grad[0] += g[0]\n",
    "                            grad[1] += g[1]\n",
    "                            grad[2] += g[2]\n",
    "                actor.train(grad)\n",
    "            else: #Pretrain Critic 用\n",
    "                out, loss, _ = critic.train([inputs], [lenth], [solution])\n",
    "                totloss += loss\n",
    "        if RL_trainable:\n",
    "            actor.update_target_network()\n",
    "            if LSTM_trainable:\n",
    "                critic.update_target_network()\n",
    "        else:\n",
    "            critic.assign_target_network()\n",
    "        if (b + 1) % 500 == 0:\n",
    "            acc_test = test(sess, actor, critic, test_data, noRL= not RL_trainable)\n",
    "            acc_dev = test(sess, actor, critic, dev_data, noRL= not RL_trainable)\n",
    "            print(\"batch \",b , \"total loss \", totloss, \"----test: \", acc_test, \"| dev: \", acc_dev)\n",
    "\n",
    "\n",
    "def test(sess, actor, critic, test_data, noRL=False):\n",
    "    acc = 0\n",
    "    for i in range(len(test_data)):\n",
    "        #prepare\n",
    "        data = test_data[i]\n",
    "        inputs, solution, lenth = data['words'], data['solution'], data['lenth']\n",
    "        \n",
    "        #predict\n",
    "        if noRL:\n",
    "            out = critic.predict_target([inputs], [lenth])\n",
    "        else:\n",
    "            actions, states, Rinput, Rlenth = sampling_RL(sess, actor, inputs, critic.wordvector_find([inputs]), lenth, Random=False)\n",
    "            out = critic.predict_target([Rinput], [Rlenth])\n",
    "        if np.argmax(out) == np.argmax(solution):\n",
    "            acc += 1\n",
    "    return float(acc) / len(test_data)\n",
    "\n",
    "\n",
    "### Main ###\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config = config) as sess:\n",
    "    #model\n",
    "    critic = LSTM_CriticNetwork(sess, args.dim, args.optimizer, args.lr, args.tau, args.grained, args.maxlenth, args.dropout, word_vector) \n",
    "    actor = ActorNetwork(sess, args.dim, args.optimizer, args.lr, args.tau)\n",
    "    #print variables\n",
    "    for item in tf.trainable_variables():\n",
    "        print (item.name, item.get_shape())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #LSTM pretrain\n",
    "    if args.RLpretrain != '':\n",
    "        pass\n",
    "    elif args.LSTMpretrain == '':\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(0, 2):\n",
    "            train(sess, actor, critic, train_data, args.batchsize, args.sample_cnt, RL_trainable=False)\n",
    "            critic.assign_target_network()\n",
    "            acc_test = test(sess, actor, critic, test_data, True)\n",
    "            acc_dev = test(sess, actor, critic, dev_data, True)\n",
    "            print (\"LSTM_only \",i, \"----test: \", acc_test, \"| dev: \", acc_dev)\n",
    "            saver.save(sess, \"checkpoints/\"+args.name+\"_base\", global_step=i)\n",
    "        print (\"LSTM pretrain OK\")\n",
    "    else:\n",
    "        print (\"Load LSTM from \", args.LSTMpretrain)\n",
    "        saver.restore(sess, args.LSTMpretrain)\n",
    "    \n",
    "    print (\"epsilon\", args.epsilon)\n",
    "\n",
    "    if args.RLpretrain == '':\n",
    "        for i in range(0, 5):\n",
    "            train(sess, actor, critic, train_data, args.batchsize, args.sample_cnt, LSTM_trainable=False)\n",
    "            acc_test = test(sess, actor, critic, test_data)\n",
    "            acc_dev = test(sess, actor, critic, dev_data)\n",
    "            print(\"RL pretrain \", i, \"----test: \", acc_test, \"| dev: \", acc_dev)\n",
    "            saver.save(sess, \"checkpoints/\"+args.name+\"_RLpre\", global_step=i)\n",
    "        print (\"RL pretrain OK\")\n",
    "    else:\n",
    "        print (\"Load RL from\", args.RLpretrain)\n",
    "        saver.restore(sess, args.RLpretrain)\n",
    "\n",
    "    for e in range(args.epoch):\n",
    "        train(sess, actor, critic, train_data, args.batchsize, args.sample_cnt)\n",
    "        acc_test = test(sess, actor, critic, test_data)\n",
    "        acc_dev = test(sess, actor, critic, dev_data)\n",
    "        print (\"epoch \", e, \"----test: \", acc_test, \"| dev: \", acc_dev)\n",
    "        saver.save(sess, \"checkpoints/\"+args.name, global_step=e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
